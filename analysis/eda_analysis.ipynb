{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"newsmediabias/news-bias-full-data\")\n",
    "train_df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Bias Detection - EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "print(f'Shape: {train_df.shape}')\n",
    "print(f'Columns: {list(train_df.columns)}')\n",
    "print(f'Missing values: {train_df.isnull().sum().sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "train_df['label'].value_counts().plot(kind='bar', ax=axes[0,0], title='Bias Labels')\n",
    "train_df['sentiment'].value_counts().plot(kind='bar', ax=axes[0,1], title='Sentiment')\n",
    "train_df['dimension'].value_counts().head(8).plot(kind='bar', ax=axes[1,0], title='Top Dimensions')\n",
    "train_df['toxic'].value_counts().plot(kind='bar', ax=axes[1,1], title='Toxicity')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text analysis\n",
    "train_df['text_length'] = train_df['text'].str.len()\n",
    "train_df['word_count'] = train_df['text'].str.split().str.len()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "train_df['text_length'].hist(bins=50, ax=axes[0], alpha=0.7)\n",
    "axes[0].set_title('Text Length')\n",
    "axes[0].axvline(train_df['text_length'].mean(), color='red', linestyle='--')\n",
    "\n",
    "train_df['word_count'].hist(bins=50, ax=axes[1], alpha=0.7)\n",
    "axes[1].set_title('Word Count')\n",
    "axes[1].axvline(train_df['word_count'].mean(), color='red', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Avg text length: {train_df[\"text_length\"].mean():.0f}')\n",
    "print(f'Avg word count: {train_df[\"word_count\"].mean():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias vs Sentiment heatmap\n",
    "crosstab = pd.crosstab(train_df['label'], train_df['sentiment'], normalize='index') * 100\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(crosstab, annot=True, fmt='.1f', cmap='Blues')\n",
    "plt.title('Bias vs Sentiment (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top aspects\n",
    "plt.figure(figsize=(10, 6))\n",
    "train_df['aspect'].value_counts().head(12).plot(kind='barh')\n",
    "plt.title('Top 12 Aspects')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Unique aspects: {train_df[\"aspect\"].nunique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biased words analysis\n",
    "train_df['biased_words_count'] = train_df['biased_words'].apply(len)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "train_df['biased_words_count'].value_counts().head(8).plot(kind='bar', ax=axes[0])\n",
    "axes[0].set_title('Biased Words Count Distribution')\n",
    "\n",
    "train_df.groupby('label')['biased_words_count'].mean().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Avg Biased Words by Label')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f'Texts with biased words: {(train_df[\"biased_words_count\"] > 0).mean()*100:.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary stats\n",
    "print('=== DATASET SUMMARY ===')\n",
    "print(f'Total samples: {len(train_df):,}')\n",
    "print(f'Dimensions: {train_df[\"dimension\"].nunique()}')\n",
    "print(f'Aspects: {train_df[\"aspect\"].nunique()}')\n",
    "print(f'Toxic content: {(train_df[\"toxic\"] == 1.0).mean()*100:.1f}%')\n",
    "print(f'Identity mentions: {(train_df[\"identity_mention\"] == \"YES\").mean()*100:.1f}%')\n",
    "print('\\nBias distribution:')\n",
    "for label, count in train_df['label'].value_counts().items():\n",
    "    print(f'  {label}: {count:,} ({count/len(train_df)*100:.1f}%)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}